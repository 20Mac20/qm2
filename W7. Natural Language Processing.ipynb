{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"W7. Natural Language Processing - Student Version.ipynb","provenance":[],"collapsed_sections":["9C_kwLPljAY0","J4P2wbX2jAZL","CN8gxS-PjAZ2"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"cxouC6cfjAYL"},"source":["## Quantitative Methods 2:  Data Science and Visualisation\n","\n","## Workshop 7 : NLP Text Analysis. \n","\n","Today we'll be using the *Natural Language Tool Kit* package **nltk**, which will allow us to split (clean) text into words, parts of speech, and sentences, and plot word occurrence and frequency.\n","\n","**Aims**\n","\n","- to work with nltk and some standard corpus texts\n","- to tokenise by word and sentence\n","- to plot word occurrence and frequency\n","- to filter by parts of speech"]},{"cell_type":"markdown","metadata":{"id":"RZwtwz53jAYM"},"source":["## Downloading the Data\n","Let's grab the data we will need this week from our course website and save it into our data folder. If you've not already created a data folder then do so using the following command. \n","\n","Don't worry if it generates an error, that means you've already got a data folder."]},{"cell_type":"code","metadata":{"id":"c8OXlIf2jAYN"},"source":["#Make a ./data directory\n","!mkdir data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AeYFCcv4jAYT"},"source":["#Make a ./data/wk7 directory\n","!mkdir data/wk7\n","\n","#Download the data into this directory\n","!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelA.txt -o ./data/wk7/PanelA.txt\n","!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelA2.txt -o ./data/wk7/PanelA2.txt\n","!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelB.txt -o ./data/wk7/PanelB.txt\n","!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelC.txt -o ./data/wk7/PanelC.txt\n","!curl https://s3.eu-west-2.amazonaws.com/qm2/wk7/PanelD.txt -o ./data/wk7/PanelD.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXB0zCQEjAYY"},"source":["import pylab\n","%matplotlib inline\n","pylab.rcParams['figure.figsize'] = (10., 8.)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAVvtEX6jAYb"},"source":["## Dirty Words\n","\n","Text often comes 'unclean' either containing tags such as HTML (or XML), or has other issues, but fortunately we will be using 'clean' sources, at least initially. Be cautious when committing to a text analysis project - you may spend a great deal of time tidying up your text.\n","\n","The kind of analysis we will be doing reqires *tokenizing* a text, and *tagging* individual words. Tokenizing means splitting the text into individul sentences or individual words, while tagging means classifying each word according to a POS (Parts Of Speech) classification. "]},{"cell_type":"markdown","metadata":{"id":"V2uw3HT0jAYb"},"source":["## The Castle of Aaargh\n","We will first experiment with nltk and its built in corpus texts. We'll work with some Monty Python, beloved of comedy bores for half a century"]},{"cell_type":"markdown","metadata":{"id":"nMgq_w5CjAYc"},"source":["## Setup\n","\n","- install nltk through package manager, or the command line\n","- import nltk\n","- type nltk.download('book'). This will automatically download the books into our workspace"]},{"cell_type":"code","metadata":{"id":"X14YfA2ejAYd"},"source":["#nltk: natural language processing toolkit\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ao7y2Gd_jAYf"},"source":["#Download the sample books\n","nltk.download('book')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BaD9wjhzjAYi"},"source":["Now, we import the sample texts. You'll notice that text6 is \"Monty Python and the Holy Grail\", as promised. Presumably this was compiled pre-*Spamalot*."]},{"cell_type":"code","metadata":{"id":"qsoVoCHejAYj"},"source":["#Import all features from nltk.book\n","from nltk.book import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bvnDgXfgjAYl"},"source":["Let's look at the object text6; we can look at the first few words.."]},{"cell_type":"code","metadata":{"id":"uV0fZDKvjAYm"},"source":["#View the first ten words of text6\n","text6[1:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vxGMtiBBjAYo"},"source":["Which are presumably stage directions rather than dialogue. How many words/symbols are in the text?"]},{"cell_type":"code","metadata":{"id":"0D3vDc8xjAYp"},"source":["#View the length of the list\n","len(text6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BRw32-UFjAYr"},"source":["Bear in mind that a lot of the functions we will carry out rely on this being a text object - we'll start to think about how we use free text and convert it to a **Text** object later."]},{"cell_type":"markdown","metadata":{"id":"unXAzIInjAYr"},"source":["We can now start to do some slightly more sophisticated work; for example, a dispersion plot to see where words appear. Let's give it a few keywords that those familiar with *...The Holy Grail* might recognise:"]},{"cell_type":"code","metadata":{"id":"dsaM5FEljAYs"},"source":["text6.dispersion_plot(['Grail','rabbit','Knights','Ni','castle'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAu_-1JEjAYt"},"source":["And we can easily count *how many* times a word appears."]},{"cell_type":"code","metadata":{"id":"upaRb9iwjAYu"},"source":["text6.count('Ni')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t2AACARjjAYx"},"source":["Or the words which most commonly appear together:"]},{"cell_type":"code","metadata":{"id":"Y4mNYR8KjAYx"},"source":["text6.collocations()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9C_kwLPljAY0"},"source":["## Exercise: From Hell's Heart I Stab at Thee\n","From Moby Dick, find out \n","- Where the narrator Ishmael, Captain Ahab and his Nemesis are mentioned. When do each enter the story? Where do they have most emphasis?\n","- Which parts of the books appear to take place at sea, and points where their ship is wrecked or sinking (spoilers)\n","- two significant places in the story (HINT: use collocations)"]},{"cell_type":"markdown","metadata":{"id":"JmkkXMhZjAY0"},"source":["Let's now look at carrying out the full process of importing and working with text data. \n","\n","## Making an IMPACT\n","\n","As part of the REF2014 exercise, universities reported on the *Impact* their research activities had on the world. Their *Impact Case Studies* were subsequently made available by HEFCE. What sort of information do they contain? How do universities frame \"impact\"? All of this data is available via the REF website.\n","\n","A little context: I've included examples from the four *panels* used by HEFCE. Broadly speaking, Panel A is health, bioscience and medicine, B is physical science and engineering, C is social science, and D is humanities - the full categories are visible here: \n","http://www.ref.ac.uk/panels/unitsofassessment/"]},{"cell_type":"markdown","metadata":{"id":"l_jPhxxDjAY1"},"source":["Let's first look at random sample from Panel A:"]},{"cell_type":"code","metadata":{"id":"OqMQQwwgjAY1"},"source":["#Data path to file\n","data_path = \"./data/wk7/PanelA.txt\"\n","\n","with open(data_path) as file:\n","    data = file.read()\n","print(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKKodS7djAY4"},"source":["We could tokenize this into sentences:"]},{"cell_type":"code","metadata":{"id":"uEVi_SoQjAY4"},"source":["sentences = nltk.sent_tokenize(data)\n","sentences[1:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Gg-XygQjAY6"},"source":["Or into individual words; generally, it may be useful to retain sentences, so we can see where two words are in the same sentence, for example - but we'll be doing something simpler:"]},{"cell_type":"code","metadata":{"id":"M5Plm1ArjAY6"},"source":["tokens = nltk.word_tokenize(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qBKQuBRVjAY9"},"source":["tokens[1:20]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEu4tpJAjAY_"},"source":["Tokenising is a process which has many subtleties and corner-cases, and you may want to proceed in a more fine-grained way for some texts:\n","\n","http://nltk.org/api/nltk.tokenize.html"]},{"cell_type":"markdown","metadata":{"id":"svVHPwxWjAZA"},"source":["Let's now convert this into a Text object, which will allow us to analyse other aspects of the text. For example, we can look at **collocations**, words which commonly appear together. This may help to provide context."]},{"cell_type":"code","metadata":{"id":"4_A0kZTtjAZA"},"source":["simple_text = nltk.Text(tokens)\n","simple_text.collocations()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mdFk6ccAjAZC"},"source":["We can create a dispersion plot - although in this case, it tells us a limited amount..."]},{"cell_type":"code","metadata":{"id":"FRf7MuRcjAZC"},"source":["simple_text.dispersion_plot(['NHS', 'evidence', 'practice', 'Hospital'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uQixTRFfjAZE"},"source":["Even from this, we get a sense of the work this unit does, and its impacts on the world. But what are the most 20 common words used? To find this out, we produce a **Freq**uency **Dist**ribution (*FreqDist*) object. This has an implicit loop - the *for* statement is telling python to look through all the words in 'tokens' and seeing how often they occur. The .lower() command converts them all to lower case for comparison, so it will flag up upper *and* lower case occurrences of the word."]},{"cell_type":"code","metadata":{"id":"NWurW51GjAZE"},"source":["fd = nltk.FreqDist(word.lower() for word in tokens)\n","fd.plot(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmvdZO9ojAZG"},"source":["Not very helpful - this includes all kinds of junk, and tells us that \"and\" is very common. Not very interesting. Let's try a bit harder and identify Parts of Speech."]},{"cell_type":"markdown","metadata":{"id":"_oLLYkCMjAZG"},"source":["## POS\n","\n","Parts of speech indicate whether something is a noun, a verb, adjective, and so on. In nltk, we can use the *pos_tag* command, which will identify which word belongs to which part of speech."]},{"cell_type":"code","metadata":{"id":"y3BhEWJGjAZH"},"source":["tagged = nltk.pos_tag(tokens)\n","tagged[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4GQoRtbijAZJ"},"source":["'NNP' refers to Proper Noun, Singular; you can find the full list of Parts of Speech here: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"]},{"cell_type":"code","metadata":{"id":"RkOv1GmojAZJ"},"source":["permitted_tags = set([\n","    'NN',\n","    'NNS'\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4P2wbX2jAZL"},"source":["## One FOR All\n","We've so far managed to avoid this staple of programming, the FOR loop - and we're not going to delve too deeply into it in the last class of term. Of course, you probably came across FOR loops and IF statements when you worked through the prerequisites for the module, but that feels like a long time ago... \n","\n","We do use FOR and IF here, and it's worth understanding a bit about what it means, even if you don't intend to use it a lot yourself. In the next piece of code, we set up *fd*, a new object which will record frequency distribution information. Then we use a FOR loop\n","\n","`for bit in tagged:\n","    ...'\n","\n","This goes through every element of tagged one at a time - and each element is called 'bit' for the purposes of this loop. Then, for each 'bit', we check that it has one of the permitted tags, and make sure it's at least 3 characters long - shorter words probably aren't all that relevant in this case:\n","\n","`if bit[1] in permitted_tags and len(bit[0])>2:'\n","\n","note that the *and* means both of these have to be true - if both *are* true, only then does the following statement execute:\n","\n","`fd[bit[0]] = fd[bit[0]] + 1'\n","\n","which increases the count for that word. So, this code increases the count for a word iff (if and only if) its at least 3 characters long, and it's of the correct tag (Noun, Singular or Plural)."]},{"cell_type":"markdown","metadata":{"id":"4LmuzZYLjAZM"},"source":["## Double Indentity\n","One final remark: we haven't dealt with **indents** much in python, but indenting the code like below, after the for statement, and *again* after the if statement, is the way that python knows it's dealing with a loop (for) and a conditional (if). It's also the way python deals with defining new functions, but that's not something you will need to do. This is just a pointer - if your code doesn't work, check the colons are there (:) and the indenting is too."]},{"cell_type":"markdown","metadata":{"id":"q4xfJbyajAZN"},"source":["On with the show - as promised, this creates a word frequency graph of nouns:"]},{"cell_type":"code","metadata":{"id":"rqnTIL3SjAZN"},"source":["fd = nltk.FreqDist()\n","\n","for bit in tagged:\n","    if bit[1] in permitted_tags and len(bit[0])>2:\n","        fd[bit[0]] = fd[bit[0]] + 1\n","        \n","fd.plot(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6E8D48iQjAZP"},"source":["We start to get a sense of the impact - 'care', 'practice', and 'services' all feature heavily."]},{"cell_type":"markdown","metadata":{"id":"IUNEIrKkjAZP"},"source":["Let's now look at another randomly chosen example from Panel A:"]},{"cell_type":"code","metadata":{"id":"KTcFZc5ejAZQ"},"source":["data_path = \"./data/wk7/PanelA2.txt\"\n","\n","with open(data_path) as file:\n","    data = file.read()\n","tokens = nltk.word_tokenize(data)\n","simple_text.collocations()\n","simple_text = nltk.Text(tokens)\n","tagged = nltk.pos_tag(tokens)\n","fd = nltk.FreqDist()\n","\n","for bit in tagged:\n","    if bit[1] in permitted_tags and len(bit[0])>2:\n","        fd[bit[0]] = fd[bit[0]] + 1\n","        \n","fd.plot(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VeqxzqwkjAZS"},"source":["A very different set of words, clearly geared towards language therapy, and working directly with patients. Perhaps if we looked at the *slightly* less common words, we'd see links between these submissions -  for example, we see *communication* appearing in both. Not a huge surprise, if we're talking about public impact, but students of the public role of the university might start to wonder about the distinctions between *communication* and *engagement*."]},{"cell_type":"markdown","metadata":{"id":"nJFweobvjAZS"},"source":["## Exercise\n","Repeat this for the examples from Panels B, C and D - what trends and keywords appear? What use do the collocations have? What do different parts of speech (e.g. verbs or proper nouns) tell you about the text?"]},{"cell_type":"markdown","metadata":{"id":"ecSgp5_EjAZT"},"source":["If we wanted to analyse the sector as a whole, we would want to analyse Impact statements en masse - and we would hope that this would draw out links across differnt statements from different centres and universities, and even in different panels."]},{"cell_type":"markdown","metadata":{"id":"CEt_3piEjAZT"},"source":["## Working with larger text datasets\n","Working with larger text corpora starts to get slow. At this point, we will look at a body of text we have previously tagged up.  The file is \"The Nameless City\" by H. P. Lovecraft, a horror author from the early 20th century."]},{"cell_type":"code","metadata":{"id":"ZW6nG-2cjAZU"},"source":["import pickle\n","import requests\n","from urllib.request import urlopen"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6p574eddjAZV"},"source":["This may take a little while - so wait for the task to run:"]},{"cell_type":"code","metadata":{"id":"XdWBrdj6jAZW"},"source":["# Loading the tokenized and tagged file. \n","tagged = pickle.load(urlopen(\"https://s3.eu-west-2.amazonaws.com/qm2/wk7/lovecraft_tagged.pickle\"), encoding='latin1')\n","\n","#How many sentences do we have?\n","len(tagged)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HtufCPFojAZY"},"source":["This is tokenised by sentence - and there are 18,513 of them. That would have taken a long time to tag up. If you're interested, this is how you take a set of *sentences* and tag them with Part of Speech:"]},{"cell_type":"code","metadata":{"id":"xLb8JynVjAZY"},"source":["exampleSentences = nltk.sent_tokenize(data)\n","  \n","exampleTagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sentences]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ooTOo3z8jAZa"},"source":["Note that the second line is running an implicit for loop through every sentence, and tagging each word."]},{"cell_type":"code","metadata":{"id":"Cz4IfCCPjAZa"},"source":["# first sentence.\n","exampleTagged[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jSuvJ_TOjAZc"},"source":["An impressive start! Let's again build our word frequency chart. Note now that we have an extra layer of FOR - we need to look at each sentence in the text; at each word in each sentence; and then check each word to see whether it is of an allowed type."]},{"cell_type":"code","metadata":{"id":"UN0d_QXCjAZc"},"source":["fd = nltk.FreqDist()\n","\n","permitted_tags = set([\n","    'JJS',\n","    'FW',\n","    'NN',\n","    'NNS',\n","    'NNP',\n","    'NNPS',\n","    'UH',\n","])\n","for sentence in tagged:\n","    for word in sentence:\n","        if word[1] in permitted_tags:\n","                fd[word[0]] = fd[word[0]] + 1\n","fd.plot(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlvchpMQjAZe"},"source":["Now that we've produced counts for all words which conform to our list of tags, we can quickly see how frequently common words appear; because we have tokenized by sentence, we have to do this with a slightly different mechanism - run through the words of interest and see how many occurrences appear in the Frequency Distribution object, fd. Again, we're sneaking in a FOR loop to run through these."]},{"cell_type":"code","metadata":{"id":"xkTMTA5bjAZf"},"source":["for word in ['space', 'nameless', 'mad', 'dread', 'fear', 'cthulhu', 'necronomicon', 'caring']:\n","    print(word, fd[word])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1-5LR5_jAZi"},"source":["So far, we've completely avoided the use of pandas - but we can put this data into a pandas dataframe very easily, and use the built-in graphing methods to change the style of our graph. \n","\n","We feed in fd.keys() - the words - and fd.values(), the wordcount."]},{"cell_type":"code","metadata":{"id":"kJpXxOLEjAZj"},"source":["#Convert to list so subscriptable\n","list(fd.keys())[1:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5WjIv1rjAZm"},"source":["#Convert to list so subscriptable\n","list(fd.values())[1:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrgMyHThjAZp"},"source":["import pandas as pd\n","\n","#Convert to list for use in creating dataframe\n","df = pd.DataFrame({'items': list(fd.keys()), 'counts': list(fd.values())})\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6cce-T97jAZu"},"source":["Let's now arrange them in order of appearance - the most common at the top."]},{"cell_type":"code","metadata":{"id":"OcIU8K-hjAZv"},"source":["df = df.sort_values(by='counts',ascending=False)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WlTkVsJcjAZx"},"source":["We now have a DataFrame with certain word tags sorted by decreasing frequency. Let's plot this in a bar graph; we will use df[1:50] to select the most common 50 words."]},{"cell_type":"code","metadata":{"id":"rfDuFypGjAZx"},"source":["import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uy8neOGijAZ0"},"source":["plt.style.use('ggplot')\n","df[1:50].plot(kind='bar', x='items', y='counts', legend=False)\n","plt.xlabel('Word')\n","plt.ylabel('Word Count')\n","plt.title('Word counts for H.P. Lovecraft\\'s \\\"The Nameless City\\\"')\n","plt.axhline(df['counts'].mean(), color='#2222ff')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CN8gxS-PjAZ2"},"source":["## Exercise\n","What is the percentage of words that appear exactly once in the entire text? (Hint : FreqDist objects have a method called 'hapaxes')"]},{"cell_type":"markdown","metadata":{"id":"1wg_R8JHjAZ3"},"source":["## Exercise\n","The words of our ex Prime Minister, David Cameron:\n","\n","    1. Select one of Cameron's speeches.\n","    2. Sentence and word tokenize it.\n","    3. POS tag it.\n","    4. Create noun and adjective histograms of the 20 most frequent words.\n","    (notice that there are several POS for each.)\n","    5. Create dispersion plots for these nouns and adjectives. \n","    6. What is the percentage of words that are adjectives in the speech?"]},{"cell_type":"code","metadata":{"id":"g-FByBrrjAZ3"},"source":["# read a raw text from a remote location. \n","speech = requests.get('https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006a.txt').text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K9zFG5MUjAZ6"},"source":["### Speeches\n","\n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-election-victory-speech-2010.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006a.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2006b.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2007.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2008.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2013.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2012.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2011.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2010.txt  \n","https://s3.eu-west-2.amazonaws.com/qm2/wk7/speeches/cameron-leaders-speech-2009.txt  "]},{"cell_type":"code","metadata":{"id":"G-YtZHxSjAZ6"},"source":["speech"],"execution_count":null,"outputs":[]}]}